{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2279c8d9",
   "metadata": {},
   "source": [
    "# Graph Neural Network for Stress Prediction on FEA Mesh Data\n",
    "This notebook trains a Graph Convolutional Network (GCN) to predict **von Mises stress** from Finite Element Analysis (FEA) mesh data, using randomized Young’s modulus scalar fields `E` as input features.  \n",
    "The model is trained on GPU (if available) for faster training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe9d2243-9915-4674-86e9-dedd5ad5bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.data import Data, Dataset, InMemoryDataset, DataLoader as GeoDataLoader\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    from torch_geometric.nn import GCNConv\n",
    "    GEOMETRIC_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    GEOMETRIC_AVAILABLE = False\n",
    "    print('torch_geometric not available. GNN-related cells will show instructions to install it.')\n",
    "    raise ImportError(\"torch_geometric is required to run this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041bb481",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set up dataset paths, split ratios, batch size, training parameters, and device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65889a-9585-4f29-bd9d-9bdac7a0c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = \"../data/DataSet1000/processed_data\"\n",
    "MESH_META_PATH = os.path.join(PROCESSED_DIR, \"mesh_metadata.npz\")\n",
    "SPLIT_RATIOS = (0.7, 0.15, 0.15)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abad208",
   "metadata": {},
   "source": [
    "## Load Single Graph Function\n",
    "Defines a helper to load one graph sample (features, edges, and target) from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9879c19-7714-4af2-8534-21b42262fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(sample_path, mesh_meta_path):\n",
    "    meta = np.load(mesh_meta_path)\n",
    "    coords = meta[\"coords\"].astype(np.float32)\n",
    "    edge_index = meta[\"edge_index\"].astype(np.int64)\n",
    "    arr = np.load(sample_path)\n",
    "    E = arr[\"E\"].astype(np.float32)\n",
    "    y = arr[\"y\"].astype(np.float32)\n",
    "\n",
    "    x = np.concatenate([coords, E.reshape(-1, 1)], axis=1).astype(np.float32)\n",
    "\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y.reshape(-1, 1))\n",
    "\n",
    "    return Data(\n",
    "        x=x,\n",
    "        edge_index=torch.from_numpy(edge_index),\n",
    "        y=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9a0b1",
   "metadata": {},
   "source": [
    "## MeshDataset Class\n",
    "Custom dataset class for loading mesh graph samples and applying normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffe9c48c-ccc9-4456-9ad7-1e55ee0b39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshDataset(Dataset):\n",
    "    def __init__(self, files, mesh_meta_path, x_mean=None, x_std=None, y_mean=None, y_std=None):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.mesh_meta_path = mesh_meta_path\n",
    "        self.x_mean = x_mean\n",
    "        self.x_std = x_std\n",
    "        self.y_mean = y_mean\n",
    "        self.y_std = y_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = load_graph(self.files[idx], self.mesh_meta_path)\n",
    "\n",
    "        if self.x_mean is not None and self.x_std is not None:\n",
    "            data.x = (data.x - self.x_mean) / self.x_std.clamp(min=1e-8)\n",
    "        if self.y_mean is not None and self.y_std is not None:\n",
    "            data.y = (data.y - self.y_mean) / self.y_std.clamp(min=1e-8)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457cd0f",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "Collect all sample files, shuffle them, and split into train/val/test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a4d956d-eef5-4f8a-bcc8-67da47943be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples total: 1000 | train: 700 | val: 150 | test: 150\n"
     ]
    }
   ],
   "source": [
    "sample_files = sorted(\n",
    "    f for f in os.listdir(PROCESSED_DIR)\n",
    "    if f.startswith(\"sample_\") and f.endswith(\".npz\")\n",
    ")\n",
    "sample_files = [os.path.join(PROCESSED_DIR, f) for f in sample_files]\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(sample_files)\n",
    "\n",
    "n_total = len(sample_files)\n",
    "n_train = int(n_total * SPLIT_RATIOS[0])\n",
    "n_val = int(n_total * SPLIT_RATIOS[1])\n",
    "\n",
    "train_files = sample_files[:n_train]\n",
    "val_files = sample_files[n_train:n_train + n_val]\n",
    "test_files = sample_files[n_train + n_val:]\n",
    "\n",
    "print(f\"Samples total: {n_total} | train: {len(train_files)} | val: {len(val_files)} | test: {len(test_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89213847",
   "metadata": {},
   "source": [
    "## Compute Normalization Parameters\n",
    "Calculate feature and target mean/std using training data for normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe8f4488-347c-4b31-a5f7-4ad0b32e1a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Mean: tensor([-3.0867e-03, -2.0257e-04,  9.9990e-02,  2.4715e+01])\n",
      "Feature Std Dev: tensor([ 0.7335,  0.7335,  0.0773, 12.4010])\n",
      "Target Mean: tensor([367930.9688])\n",
      "Target Std Dev: tensor([14710574.])\n",
      "Normalization parameters saved to normalization_params.pth\n"
     ]
    }
   ],
   "source": [
    "all_x = []\n",
    "all_y = []\n",
    "\n",
    "if len(train_files) == 0:\n",
    "    raise RuntimeError(\"Train file list is empty. Cannot compute normalization stats.\")\n",
    "\n",
    "for file in train_files:\n",
    "    data = load_graph(file, MESH_META_PATH)\n",
    "    if data.x.numel() == 0 or data.y.numel() == 0:\n",
    "        continue  # to skip empty data\n",
    "    all_x.append(data.x)\n",
    "    all_y.append(data.y)\n",
    "\n",
    "if len(all_x) == 0 or len(all_y) == 0:\n",
    "    raise RuntimeError(\"No valid training data found to compute normalization parameters.\")\n",
    "\n",
    "all_x = torch.cat(all_x, dim=0)\n",
    "all_y = torch.cat(all_y, dim=0)\n",
    "\n",
    "x_mean = torch.mean(all_x, dim=0)\n",
    "x_std = torch.std(all_x, dim=0)\n",
    "y_mean = torch.mean(all_y, dim=0)\n",
    "y_std = torch.std(all_y, dim=0)\n",
    "\n",
    "print(\"Feature Mean:\", x_mean)\n",
    "print(\"Feature Std Dev:\", x_std)\n",
    "print(\"Target Mean:\", y_mean)\n",
    "print(\"Target Std Dev:\", y_std)\n",
    "\n",
    "normalization_params = {\n",
    "    'x_mean': x_mean,\n",
    "    'x_std': x_std,\n",
    "    'y_mean': y_mean,\n",
    "    'y_std': y_std\n",
    "}\n",
    "\n",
    "torch.save(normalization_params, \"normalization_params.pth\")\n",
    "print(\"Normalization parameters saved to normalization_params.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb23916",
   "metadata": {},
   "source": [
    "## Create Datasets and DataLoaders\n",
    "Instantiate training, validation, and test datasets and wrap them with loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3553ce28-46ab-428f-92fb-ad26a961de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MeshDataset(train_files, MESH_META_PATH, x_mean, x_std, y_mean, y_std)\n",
    "val_dataset = MeshDataset(val_files, MESH_META_PATH, x_mean, x_std, y_mean, y_std)\n",
    "test_dataset = MeshDataset(test_files, MESH_META_PATH, x_mean, x_std, y_mean, y_std)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c4fa4",
   "metadata": {},
   "source": [
    "## GCN Model\n",
    "Define the Graph Convolutional Network (GCN) architecture for regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a557e71-ae6b-4b9a-b68c-478e5c62daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = GCNConv(hidden_channels, 128)\n",
    "        self.conv3 = GCNConv(128, 64)\n",
    "        self.fc1 = torch.nn.Linear(64, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, 32)\n",
    "        self.fc5 = torch.nn.Linear(32, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "def build_gnn(in_channels, hidden_channels, out_channels):\n",
    "    return GCN(in_channels, hidden_channels, out_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0bc857",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "Defines one training epoch: forward pass, loss computation, and backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "446a7587-1630-449a-a597-e50517608889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = loss_fn(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335cb9c",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "Evaluates the model on a dataset, returning MAE, MSE, and R² scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d07c37a5-dc42-4486-b601-a3765471a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    normalization_params = torch.load(\"normalization_params.pth\")\n",
    "    y_mean = normalization_params['y_mean']\n",
    "    y_std = normalization_params['y_std']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            pred = model(batch.x, batch.edge_index)\n",
    "\n",
    "            pred_cpu = pred.cpu()\n",
    "            true_cpu = batch.y.cpu()\n",
    "\n",
    "            denorm_pred = pred_cpu * y_std + y_mean\n",
    "            denorm_true = true_cpu * y_std + y_mean\n",
    "\n",
    "            y_pred.append(denorm_pred.numpy().flatten())\n",
    "            y_true.append(denorm_true.numpy().flatten())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"R2\": r2_score(y_true, y_pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410b618",
   "metadata": {},
   "source": [
    "## Run Training and Validation\n",
    "Train the model for multiple epochs and evaluate on validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a9ccc-2466-4d9c-b5e7-cf21aa05f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_dim = train_dataset[0].x.shape[1]\n",
    "model = build_gnn(sample_input_dim, 128, 1).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(model, train_loader, optimizer, loss_fn)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train MSE: {loss:.6f}\")\n",
    "\n",
    "metrics = evaluate(model, val_loader)\n",
    "print(\"\\nValidation:\", metrics)\n",
    "\n",
    "torch.save(model.state_dict(), \"gnn_model_normalized.pth\")\n",
    "print(\"✅ Model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU-Accelerated ML Surrogate for FEA Stress)",
   "language": "python",
   "name": "gpu_feasurrogate_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
